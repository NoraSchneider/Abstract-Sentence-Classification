{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykL-2goqyxCy"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIcrD9vUy1np"
      },
      "source": [
        "## Set up packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_-9xfHnvW9p"
      },
      "outputs": [],
      "source": [
        "# Add current position to path in order to use project2Library\n",
        "import sys\n",
        "sys.path.append(\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX83Xo72BTtl"
      },
      "source": [
        "## Data Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUrRsSs4zjZ6"
      },
      "outputs": [],
      "source": [
        "# Data Parameters\n",
        "small = True\n",
        "replace_num=True\n",
        "\n",
        "# BERT Parameters\n",
        "num_labels = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr3Dj1JjBbVT"
      },
      "source": [
        "## Set up Folder Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEQcOJEizRaE"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Set up data path\n",
        "data_dir = Path(\"./data\")\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Set up model path\n",
        "model_base_dir = Path(\"./TrainedModels\")\n",
        "if not model_base_dir.exists():\n",
        "    raise Exception(\"You must first train the base model before creating the hierarchical model\")\n",
        "\n",
        "# Define one folder for full experiment\n",
        "model_dir_final = model_base_dir.joinpath(\"KD\")\n",
        "model_dir_final.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYOEBO-U_wkT"
      },
      "source": [
        "# Pre-Process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eIhfywb_2XH"
      },
      "source": [
        "## Download & Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHnUGEdCzMOC"
      },
      "outputs": [],
      "source": [
        "from project2Lib import download_data, load_data, load_data_as_dataframe\n",
        "\n",
        "# We download the data if nessecery\n",
        "download_data(data_dir=data_dir, small=small, replace_num=replace_num)\n",
        "\n",
        "# We load the data as a dataframe\n",
        "dataset = load_data_as_dataframe(data_dir=data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK0qNS1s_7Mk"
      },
      "source": [
        "## Set up TF-IDF Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfEPo3BIz4mx"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Fit the TF-IDF-model\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_features=50000, ngram_range=(1,2))\n",
        "X = tfidf_vectorizer.fit(dataset[0][\"sentence\"].values)\n",
        "\n",
        "# Create the embedding\n",
        "X = {\n",
        "    k: tfidf_vectorizer.transform(dataset[i][\"sentence\"].values) \\\n",
        "    for i, k in enumerate([\"train\", \"dev\", \"test\"])\n",
        "}\n",
        "for k in X:\n",
        "    X[k].sort_indices()\n",
        "\n",
        "# Load the labels\n",
        "Y = {\n",
        "    k: dataset[i][\"label\"].to_numpy() \\\n",
        "    for i, k in enumerate([\"train\", \"dev\", \"test\"])\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDW9A2UM1P3O"
      },
      "source": [
        "## Load Data for Getting Teacher Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HERIyBs00bAe"
      },
      "outputs": [],
      "source": [
        "from project2Lib import load_embedded, download_data\n",
        "import torch\n",
        "\n",
        "# Load the embedded data from the BERT model\n",
        "load_args = {\n",
        "    \"dataset_path\": data_dir.joinpath(\"dataset_small_bert\"),\n",
        "    \"embedding\": \"bert\",\n",
        "    \"model_checkpoint\": model_base_dir.joinpath(\"bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_nofreeze_200k/final_model/pytorch_model.bin\"),\n",
        "    \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "# Assert that model is already trained\n",
        "if not load_args[\"model_checkpoint\"].exists():\n",
        "    raise Exception(\"You must first train the base model before using the embedding\")\n",
        "\n",
        "# We download the data if nessecery\n",
        "download_data()\n",
        "\n",
        "# We load the data as a Huggingface-dataset\n",
        "encoded_dataset = load_embedded(data_dir=data_dir, fields=None, group_by_abstracs=True, **load_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke3GmeFY1XVp"
      },
      "source": [
        "## Get Teacher Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blk-gLgf-3Aj"
      },
      "outputs": [],
      "source": [
        "from project2Lib import SentenceClassifier, SentenceCollator\n",
        "\n",
        "# Load teach\n",
        "teacher_path = model_base_dir.joinpath(load_args[\"embedding\"]).joinpath(\"final_model/pytorch_model.bin\")\n",
        "\n",
        "# Assert that techer model is already trained\n",
        "if not load_args[\"model_checkpoint\"].exists():\n",
        "    raise Exception(\"You must first train the teacher model before doing KD\")\n",
        "\n",
        "# Load pre-trained teacher\n",
        "model = SentenceClassifier(num_labels, 768, None)\n",
        "model.load_state_dict(torch.load(teacher_path, map_location=load_args[\"device\"]))\n",
        "\n",
        "# Add logits to the dataset\n",
        "logit_dataset = encoded_dataset.map(\n",
        "    lambda batch: {\"logits\": model(**SentenceCollator()([batch])).logits},\n",
        "    batched = False\n",
        ")\n",
        "\n",
        "# Convert to numpy array\n",
        "teacher_logits = {\n",
        "    k: np.array([x for s in v[\"logits\"] for x in s])  for k, v in logit_dataset.items()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYptNLlm3cZY"
      },
      "source": [
        "# Run Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNh5Vdej3ezc"
      },
      "source": [
        "## Define KD-opbjective for Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-G6Fj553a0I"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from project2Lib import add_KD\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def objective(trial, base_model, num_labels, model_dir, X, teacher_logits, Y):\n",
        "    \n",
        "    # Get parameters for trial\n",
        "    alpha = trial.suggest_float(\"alpha\", 0, 1)\n",
        "    T = trial.suggest_float(\"T\", 0, 1)\n",
        "\n",
        "    # Print status\n",
        "    print(\"New trial: \", alpha, T)    \n",
        "\n",
        "    # Get KD-model with given parameters\n",
        "    model = add_KD(alpha, T, base_model, num_labels)\n",
        "\n",
        "    # Set up model\n",
        "    file_path = model_dir.joinpath(\"./model.h5\")\n",
        "    checkpoint = ModelCheckpoint(file_path, monitor=\"val_accuracy\", verbose=0, save_best_only=True, mode='max')   \n",
        "    early = EarlyStopping(monitor=\"val_accuracy\", patience=10)\n",
        "    redonplat = ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", patience=3, verbose=0)\n",
        "    callbacks_list = [checkpoint, early, redonplat]\n",
        "\n",
        "    # Run model\n",
        "    model.fit([X[\"train\"], teacher_logits[\"train\"]], Y[\"train\"], \n",
        "              validation_data=([X[\"dev\"], teacher_logits[\"dev\"]], Y[\"dev\"]), \n",
        "              epochs=100, batch_size=1024, verbose=0, callbacks=callbacks_list)\n",
        "    \n",
        "    # Load best state\n",
        "    model.load_weights(file_path)\n",
        "\n",
        "    # Compute F1\n",
        "    preds = model.predict([X[\"dev\"], teacher_logits[\"dev\"]])\n",
        "    preds = np.argmax(preds[:, :num_labels], axis=-1)\n",
        "    score = f1_score(Y[\"dev\"], preds, average=\"weighted\")\n",
        "\n",
        "    print(\"Finished trial: \", alpha, T, score)\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0yXnN0B9XW8"
      },
      "source": [
        "## Define Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb_FrJfG9ao1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def get_logistic_regression(num_labels = 5, numb_features=50000):\n",
        "    lr = Sequential()\n",
        "    lr.add(Dense(num_labels,input_dim = numb_features))\n",
        "    lr.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AvSuhO49zv3"
      },
      "source": [
        "## Run Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOkgwLcV8zp5"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "# HP-space for KD\n",
        "def my_hp_space(trial):\n",
        "    return {\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n",
        "        \"T\": trial.suggest_float(\"T\", 0.5, 5)\n",
        "    }\n",
        "\n",
        "# Optimize the hyperparameters for KD\n",
        "study = optuna.create_study() \n",
        "best_trial = study.optimize(\n",
        "    lambda trial: objective(trial, get_logistic_regression, num_labels, model_dir_final, X, teacher_logits, Y), \n",
        "    n_trials=30\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPIFImF7-LBg"
      },
      "source": [
        "## Run Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPKYNMWX-M20"
      },
      "outputs": [],
      "source": [
        "# Get parameters for trial\n",
        "alpha = best_trial.params[\"alpha\"]\n",
        "T = best_trial.params[\"T\"]\n",
        "\n",
        "# Print status\n",
        "print(\"Training final model with parameters: \", alpha, T)    \n",
        "\n",
        "# Get KD-model with given parameters\n",
        "model = add_KD(alpha, T, get_logistic_regression, num_labels)\n",
        "\n",
        "# Set up model\n",
        "file_path = model_dir_final.joinpath(\"./model.h5\")\n",
        "checkpoint = ModelCheckpoint(file_path, monitor=\"val_accuracy\", verbose=0, save_best_only=True, mode='max')   \n",
        "early = EarlyStopping(monitor=\"val_accuracy\", patience=10)\n",
        "redonplat = ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", patience=3, verbose=0)\n",
        "callbacks_list = [checkpoint, early, redonplat]\n",
        "\n",
        "# Run model\n",
        "model.fit([X[\"train\"], teacher_logits[\"train\"]], Y[\"train\"], \n",
        "          validation_data=([X[\"dev\"], teacher_logits[\"dev\"]], Y[\"dev\"]), \n",
        "          epochs=100, batch_size=1024, verbose=0, callbacks=callbacks_list)\n",
        "\n",
        "# Load best state\n",
        "model.load_weights(file_path)\n",
        "\n",
        "# Compute F1\n",
        "preds = model.predict([X[\"test\"], teacher_logits[\"test\"]])\n",
        "preds = np.argmax(preds[:, :num_labels], axis=-1)\n",
        "f1 = f1_score(Y[\"dev\"], preds, average=\"weighted\")\n",
        "acc = accuracy_score(Y[\"dev\"], preds)\n",
        "\n",
        "# Print results\n",
        "print(\"Finished model with f1 = \", f1, \" and accuracy = \", acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TrainingKD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
