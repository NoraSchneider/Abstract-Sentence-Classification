{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainingBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ],
      "metadata": {
        "id": "lcRNtOFeUxQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up packages"
      ],
      "metadata": {
        "id": "vkBPd2sQU0oQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r1SPtMioMXW"
      },
      "outputs": [],
      "source": [
        "# Add current position to path in order to use project2Library\n",
        "import sys\n",
        "sys.path.append(\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data & Model Settings"
      ],
      "metadata": {
        "id": "FrdXG2_6U8l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Parameters\n",
        "\n",
        "# Select if to use 200k or 20k dataset\n",
        "small = input(\"Use small dataset? [yes/no]\").lower() == \"yes\"\n",
        "\n",
        "# We always replace numbers with @ as done in the original paper\n",
        "replace_num = True\n",
        "\n",
        "# BERT Parameters\n",
        "\n",
        "# Select if to use domain-specific or general purpose version of BERT\n",
        "if input(\"Use pubmed-BERT? [yes/no]\").lower() == \"yes\":\n",
        "    model_checkpoint = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "else:\n",
        "    model_checkpoint = \"bert-base-uncased\"\n",
        "\n",
        "# Select if to fine-tune BERT\n",
        "freeze_bert = input(\"Fine-tune bert? [yes/no]\").lower() == \"no\"\n",
        "\n",
        "# Number of labels is always 5\n",
        "num_labels = 5"
      ],
      "metadata": {
        "id": "HnKGvjwgjyiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Folder Structure"
      ],
      "metadata": {
        "id": "l1GFphoAVC9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Set up data path\n",
        "data_dir = Path(\"./data\")\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define one folder for full experiment\n",
        "base_path = f\"./TrainedModels/bert/{model_checkpoint}_\" \\\n",
        "            f\"{'freeze' if freeze_bert else 'nofreeze'}_{'20k' if small else '200k'}\"\n",
        "\n",
        "# Use one sub-folder for hyper paramerter tuning\n",
        "model_dir_hp = Path(base_path).joinpath(\"hp\")\n",
        "model_dir_hp.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Use another sub-folder for the final model\n",
        "model_dir_final = Path(base_path).joinpath(\"final_model\")\n",
        "model_dir_final.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "vByHhnaPLWTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Process Data"
      ],
      "metadata": {
        "id": "HviLP3g1VKbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download & Load Data"
      ],
      "metadata": {
        "id": "_SRrzPRHaSdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from project2Lib import download_data, load_data\n",
        "\n",
        "# We download the data if nessecery\n",
        "download_data(data_dir=data_dir, small=small, replace_num=replace_num)\n",
        "\n",
        "# We load the data as a Huggingface-dataset \n",
        "dataset = load_data(data_dir=data_dir)"
      ],
      "metadata": {
        "id": "UsPI0mHGpxYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize Data"
      ],
      "metadata": {
        "id": "ubfwG-YHjrO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# We use the tokenizer corresponding to the BERT-model\n",
        "# This also automatically pre-process the data by lower-casing the sentences\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, model_max_length=512)\n",
        "\n",
        "# Encode the dataset by applying the tokenizer\n",
        "encoded_dataset = dataset.map(\n",
        "    lambda x: tokenizer(x[\"sentence\"], truncation=True), \n",
        "    batched=True\n",
        ")"
      ],
      "metadata": {
        "id": "LdrRlLUskKgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Training"
      ],
      "metadata": {
        "id": "5TrS3lYnVUXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Training"
      ],
      "metadata": {
        "id": "LJ6juFZwVfJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from project2Lib import compute_f1, bert_init\n",
        "\n",
        "# Set up default training arguments\n",
        "args = TrainingArguments(\n",
        "    model_dir_hp,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"no\",\n",
        "    learning_rate=1e-3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=False,\n",
        "    metric_for_best_model = \"f1\",\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Set up trainer\n",
        "# Only train each model on a tenth of the data for performance reasons \n",
        "# during hyper parameter tuning\n",
        "trainer = Trainer(\n",
        "    model_init=lambda: bert_init(model_checkpoint, freeze_bert, num_labels),\n",
        "    args=args,\n",
        "    train_dataset=encoded_dataset[\"train\"].shard(index=1, num_shards=10),\n",
        "    eval_dataset=encoded_dataset[\"dev\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_f1\n",
        ")\n"
      ],
      "metadata": {
        "id": "_6MySlwFtoyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Run Hype-Parameter Tuning"
      ],
      "metadata": {
        "id": "D5WHZ1u5VkBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tune hyper parameters over 15 individual runs and select \n",
        "# the best performing combinations\n",
        "def my_hp_space(trial, small):\n",
        "\n",
        "    l_lr, u_lr = 1e-5, 1e-2 if small else 1e-6, 1e-4\n",
        "    l_epochs, u_epochs = 3, 8 if small else 1, 1\n",
        "\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", l_lr, u_lr, log=True),\n",
        "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", l_epochs, u_epochs),\n",
        "        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
        "    }\n",
        "\n",
        "\n",
        "# Only do hyper parameter search for small models\n",
        "n_trials = 15\n",
        "trainer.hyperparameter_search(\n",
        "    n_trials=n_trials,\n",
        "    direction=\"maximize\",\n",
        "    hp_space= lambda trial: my_hp_space(trial, small)\n",
        ")"
      ],
      "metadata": {
        "id": "xHfsZNlJPq9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Then Load Best Found Hyper-Parameters and Train Final Model"
      ],
      "metadata": {
        "id": "KbJeppYfVo2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from project2Lib import load_best_bert\n",
        "from transformers.trainer_utils import IntervalStrategy\n",
        "\n",
        "# Update trainer for final run\n",
        "_, top_args = load_best_bert(model_dir_hp)\n",
        "top_args[\"evaluation_strategy\"] = IntervalStrategy(\"epoch\")\n",
        "top_args[\"save_strategy\"] = IntervalStrategy(\"epoch\")\n",
        "top_args[\"load_best_model_at_end\"] = True\n",
        "top_args[\"output_dir\"] = model_dir_final\n",
        "top_args[\"save_total_limit\"] = 1\n",
        "\n",
        "# Only one epoch for the big models\n",
        "setattr(trainer, \"args\", TrainingArguments(**top_args))\n",
        "setattr(trainer, \"train_dataset\", encoded_dataset[\"train\"])\n",
        "\n",
        "# Run training on full dataset and save state\n",
        "trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.save_state()\n"
      ],
      "metadata": {
        "id": "gZLWP8oyN2J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Final Model and Save Results"
      ],
      "metadata": {
        "id": "ubcvGz2OVyfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Evaluate on the test dataset\n",
        "results = trainer.evaluate(encoded_dataset[\"test\"])\n",
        "\n",
        "# Save to results file\n",
        "json.dump(\n",
        "    results,\n",
        "    open(model_dir_final.joinpath(\"results.json\"), \"w+\"),\n",
        ")\n"
      ],
      "metadata": {
        "id": "60Ag07WAJgOU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}