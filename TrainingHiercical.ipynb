{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcRNtOFeUxQd"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkBPd2sQU0oQ"
      },
      "source": [
        "## Set up packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r1SPtMioMXW"
      },
      "outputs": [],
      "source": [
        "# Add current position to path in order to use project2Library\n",
        "import sys\n",
        "sys.path.append(\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrdXG2_6U8l-"
      },
      "source": [
        "## Data & Model Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnKGvjwgjyiX"
      },
      "outputs": [],
      "source": [
        "# Data Parameters\n",
        "\n",
        "# Select if to use 200k or 20k dataset\n",
        "small = input(\"Use small dataset? [yes/no]\").lower() == \"yes\"\n",
        "\n",
        "# We always replace numbers with @ as done in the original paper\n",
        "replace_num = True\n",
        "\n",
        "# Number of labels is always 5\n",
        "num_labels = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1GFphoAVC9D"
      },
      "source": [
        "## Set up Basic Folder Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vByHhnaPLWTm"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Set up data path\n",
        "data_dir = Path(\"./data\")\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Set up model path\n",
        "model_base_dir = Path(\"./TrainedModels\")\n",
        "if not model_base_dir.exists():\n",
        "    raise Exception(\"You must first train the base model before creating the hierarchical model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select Base Model and Get Corresponding Arguments"
      ],
      "metadata": {
        "id": "PXwL5Hi5yBit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from project2Lib import load_embedded, download_data\n",
        "from project2Lib import TokenCollator, TokenClassifier, SentenceCollator, SentenceClassifier\n",
        "import torch\n",
        "\n",
        "# Select if to use domain-specific or general purpose version of BERT\n",
        "if input(\"Use BERT-model? [yes/no]\").lower() == \"yes\":\n",
        "    load_args = {\n",
        "        \"dataset_path\": data_dir.joinpath(\"dataset_small_bert\"),\n",
        "        \"embedding\": \"bert\",\n",
        "        \"model_checkpoint\": model_base_dir.joinpath(\"bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_nofreeze_200k/final_model/pytorch_model.bin\"),\n",
        "        \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    }\n",
        "    model_init = lambda trial: SentenceClassifier(num_labels=num_labels, sentence_size=768, trial=trial)\n",
        "    collator = SentenceCollator()\n",
        "\n",
        "else:\n",
        "    load_args = {\n",
        "        \"dataset_path\": data_dir.joinpath(\"dataset_small_w2v\"),\n",
        "        \"embedding\": \"w2v\",\n",
        "        \"model_checkpoint\": model_base_dir.joinpath(\"w2v_200_lemmatization.bin\")        \n",
        "    }\n",
        "    model_init = lambda trial: TokenClassifier(num_labels=num_labels, token_size=200, trial=trial)\n",
        "    collator = TokenCollator()\n"
      ],
      "metadata": {
        "id": "i2w9Q3s6xlk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HviLP3g1VKbw"
      },
      "source": [
        "# Pre-Process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up final folder structure for model"
      ],
      "metadata": {
        "id": "Xl23kt-Gxvxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up final folder structure\n",
        "model_dir_hp = model_base_dir.joinpath(load_args[\"embedding\"]).joinpath(\"hp\")\n",
        "model_dir_hp.mkdir(parents=True, exist_ok=True)\n",
        "model_dir_final = model_base_dir.joinpath(load_args[\"embedding\"]).joinpath(\"final_model\")\n",
        "model_dir_final.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "5vWq59P2xpFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SRrzPRHaSdX"
      },
      "source": [
        "## Download & Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from project2Lib import load_embedded, download_data\n",
        "\n",
        "# Assert that model is already trained\n",
        "if not load_args[\"model_checkpoint\"].exists():\n",
        "    raise Exception(\"You must first train the base model before using the embedding\")\n",
        "\n",
        "# We download the data if nessecery\n",
        "download_data()\n",
        "\n",
        "# We load the data as a Huggingface-dataset\n",
        "encoded_dataset = load_embedded(data_dir=data_dir, fields=None, group_by_abstracs=True, **load_args)"
      ],
      "metadata": {
        "id": "OlMQXUIzCVj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TrS3lYnVUXh"
      },
      "source": [
        "# Run Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ6juFZwVfJ7"
      },
      "source": [
        "## Set up Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6MySlwFtoyT"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from project2Lib import compute_f1, bert_init, TokenCollator, TokenClassifier\n",
        "\n",
        "# Set up default training arguments\n",
        "args = TrainingArguments(\n",
        "    model_dir_hp,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"no\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=False,\n",
        "    metric_for_best_model = \"f1\",\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Set up trainer\n",
        "# Only train each model on a tenth of the data for performance reasons \n",
        "# during hyper parameter tuning\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"dev\"],\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_f1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5WHZ1u5VkBR"
      },
      "source": [
        "## First Run Hype-Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHfsZNlJPq9J"
      },
      "outputs": [],
      "source": [
        "# Tune hyper parameters over 15 individual runs and select \n",
        "# the best performing combinations\n",
        "def my_hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3),\n",
        "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 7),\n",
        "        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64]),\n",
        "        \"per_device_eval_batch_size\": trial.suggest_categorical(\"per_device_eval_batch_size\", [16]),\n",
        "        \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [64, 128, 256]),\n",
        "        \"dropout_p\": trial.suggest_float(\"dropout_p\", 0.1, 0.75),\n",
        "        \"num_layers\": trial.suggest_int(\"num_layers\", 1, 3),\n",
        "        \"sentence_size\": trial.suggest_categorical(\"sentence_size\", [64, 128, 256]),\n",
        "    }\n",
        "\n",
        "n_trials = 30\n",
        "trainer.hyperparameter_search(\n",
        "    n_trials=n_trials,\n",
        "    direction=\"maximize\",\n",
        "    hp_space=my_hp_space\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbJeppYfVo2i"
      },
      "source": [
        "## Then Load Best Found Hyper-Parameters and Train Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZLWP8oyN2J-"
      },
      "outputs": [],
      "source": [
        "from project2Lib import load_best_bert\n",
        "from transformers.trainer_utils import IntervalStrategy\n",
        "\n",
        "# Update trainer for final run\n",
        "_, top_args = load_best_bert(model_dir_hp)\n",
        "top_args[\"evaluation_strategy\"] = IntervalStrategy(\"epoch\")\n",
        "top_args[\"save_strategy\"] = IntervalStrategy(\"epoch\")\n",
        "top_args[\"load_best_model_at_end\"] = True\n",
        "top_args[\"output_dir\"] = model_dir_final\n",
        "setattr(trainer, \"args\", TrainingArguments(**top_args))\n",
        "setattr(trainer, \"train_dataset\", encoded_dataset[\"train\"])\n",
        "setattr(trainer, \"eval_dataset\", encoded_dataset[\"dev\"])\n",
        "\n",
        "# Run training on full dataset and save state\n",
        "trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.save_state()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubcvGz2OVyfp"
      },
      "source": [
        "## Test Final Model and Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60Ag07WAJgOU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Evaluate on the test dataset\n",
        "results = trainer.evaluate(encoded_dataset[\"test\"])\n",
        "\n",
        "# Save to results file\n",
        "json.dump(\n",
        "    results,\n",
        "    open(model_dir_final.joinpath(\"results.json\"), \"w+\"),\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TrainingHiercical.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}